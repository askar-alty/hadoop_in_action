{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MapReduce Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применяется, кода одной MapReduce задачей нельзя или невозможно или неэфективно решить задачу обработки данных.  Тогда применяется все возможное сцепление, параллельный запуск MapReduce задач, объединение выходных данных для аоследующей обработки.\n",
    "А так же сцепление различных мапперов и редьюсеров в одну MapReduce задачу. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"Hadoop Streaming\")\n",
    "hdfs = \"hdfs://localhost:9000/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример счепления мапперов и редьюсеров в одну MapReduce задачу. \n",
    "\n",
    "Пусть задача выглядит в следующим образом:  \n",
    "<h6>\n",
    "Map1  |  Map2  |  Reduce  |  Map3  |  Map4 \n",
    "</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.hadoop.conf.Configuration;\n",
    "import org.apache.hadoop.conf.Configured;\n",
    "import org.apache.hadoop.fs.Path;\n",
    "import org.apache.hadoop.io.LongWritable;\n",
    "import org.apache.hadoop.io.Text;\n",
    "import org.apache.hadoop.mapreduce.Job;\n",
    "import org.apache.hadoop.mapreduce.Mapper;\n",
    "import org.apache.hadoop.mapreduce.Reducer;\n",
    "import org.apache.hadoop.mapreduce.lib.chain.ChainMapper;\n",
    "import org.apache.hadoop.mapreduce.lib.chain.ChainReducer;\n",
    "import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n",
    "import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;\n",
    "import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n",
    "import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;\n",
    "import org.apache.hadoop.util.Tool;\n",
    "import org.apache.hadoop.util.ToolRunner;\n",
    "\n",
    "import java.io.IOException;\n",
    "\n",
    "/**\n",
    " * @author Askar Shabykov\n",
    " * @since 02.06.17\n",
    " */\n",
    "public class ChainMapReduceTask extends Configured implements Tool {\n",
    "\n",
    "    // Mapper 1\n",
    "    public static class Mapper1 extends Mapper<LongWritable, Text, Text, Text> {\n",
    "        public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n",
    "            String[] fields = value.toString().split(\",\");\n",
    "            String country = fields[4];\n",
    "            if (country.equals(\"US\") || country.equals(\"RU\") || country.equals(\"FR\") || country.equals(\"GB\")) { // фильтр по стране\n",
    "                context.write(new Text(country), value);\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "    // Mapper 2\n",
    "    public static class Mapper2 extends Mapper<Text, Text, Text, Text> {\n",
    "        public void map(Text key, Text value, Context context) throws IOException, InterruptedException {\n",
    "            String[] fields = value.toString().split(\",\");\n",
    "            Integer date = Integer.parseInt(fields[1]);\n",
    "            if (date <= 2000 && date >= 1980) {    /// фильтр по дате\n",
    "                context.write(key, value);\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "    // Reducer\n",
    "    public static class Reducer1 extends Reducer<Text, Text, Text, Text> {\n",
    "        public void reduce(Text key, Iterable<Text> values, Context context) throws IOException, InterruptedException {\n",
    "\n",
    "            double sum = 0;\n",
    "            int count = 0;\n",
    "            for (Text value : values) {\n",
    "                String[] fields = value.toString().split(\",\");\n",
    "                sum += Double.parseDouble(fields[8]);\n",
    "                count += 1;\n",
    "            }\n",
    "            context.write(key, new Text(String.valueOf(sum) + String.valueOf(count)));\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "    // Mapper 3\n",
    "    public static class Mapper3 extends Mapper<Text, Text, LongWritable, Text> {\n",
    "        public void map(Text key, Text value, Context context) throws IOException, InterruptedException {\n",
    "            String[] fields = value.toString().split(\",\");\n",
    "            Long sum = Long.parseLong(fields[0]);\n",
    "            context.write(new LongWritable(sum), new Text(key.toString() + \":\" + value.toString()));\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "    // Mapper 4\n",
    "    public static class Mapper4 extends Mapper<LongWritable, Text, LongWritable, Text> {\n",
    "        public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n",
    "            context.write(key, value);\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "    public int run(String[] args) throws IOException, ClassNotFoundException, InterruptedException {\n",
    "\n",
    "        Configuration configuration = getConf();\n",
    "        Job job = Job.getInstance(configuration, \"Cain Job\");\n",
    "\n",
    "        job.setJarByClass(this.getClass());\n",
    "        job.setInputFormatClass(TextInputFormat.class);\n",
    "        job.setOutputFormatClass(TextOutputFormat.class);\n",
    "\n",
    "        Path in_put = new Path(args[0]);\n",
    "        Path out_put = new Path(args[1]);\n",
    "\n",
    "        FileInputFormat.setInputPaths(job, in_put);\n",
    "        FileOutputFormat.setOutputPath(job, out_put);\n",
    "\n",
    "        Configuration mapConf1 = new Configuration(false);\n",
    "        ChainMapper.addMapper(job, Mapper1.class, LongWritable.class, Text.class, Text.class, Text.class, mapConf1);\n",
    "\n",
    "        Configuration mapConf2 = new Configuration(false);\n",
    "        ChainMapper.addMapper(job, Mapper2.class, Text.class, Text.class, Text.class, Text.class, mapConf2);\n",
    "\n",
    "        Configuration reduceConf1 = new Configuration(false);\n",
    "        ChainReducer.setReducer(job, Reducer.class, Text.class, Text.class, Text.class, Text.class, reduceConf1);\n",
    "\n",
    "        Configuration mapConf3 = new Configuration(false);\n",
    "        ChainReducer.addMapper(job, Mapper3.class, Text.class, Text.class, LongWritable.class, Text.class, mapConf3);\n",
    "\n",
    "        Configuration mapConf4 = new Configuration(false);\n",
    "        ChainReducer.addMapper(job, Mapper4.class, LongWritable.class, Text.class, LongWritable.class, Text.class, mapConf4);\n",
    "\n",
    "        return job.waitForCompletion(true) ? 0 : 1;\n",
    "    }\n",
    "\n",
    "    public static void main(String[] args) throws Exception {\n",
    "        int exitCode = ToolRunner.run(new Configuration(), new ChainMapReduceTask(), args);\n",
    "        System.exit(exitCode);\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " hadoop jar ChainMapReduce-1.0-SNAPSHOT.jar\t \\\n",
    "    ChainMapReduceTask \\\n",
    "    /user/askar/apat63_99.txt \\\n",
    "    /user/askar/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_data = sc.textFile(hdfs + \"/user/askar/output/part-r-00000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_data.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Да как-то впадлу писать новый пример, в принцыпе все понятно..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
