{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>\n",
    "Домашняя директория Hadoop:\n",
    "</h5>\n",
    "\n",
    "- $ export HADOOP_HOME=/usr/local/Cellar/hadoop/2.8.0\n",
    "\n",
    "<h5>\n",
    "Найдем  jar-файл hadoop datajoin:\n",
    "</h5>\n",
    "\n",
    "- $ find /usr/local/Cellar/hadoop -name hadoop-datajoin* -print\n",
    "\n",
    "<h5> \n",
    "jar-файл hadoop datajoin:\n",
    "</h5>\n",
    "\n",
    "- $HADOOP_HOME/libexec/share/hadoop/tools/lib/hadoop-datajoin-2.8.0.jar\n",
    "- /usr/local/Cellar/hadoop/2.8.0/libexec/share/hadoop/tools/lib/hadoop-datajoin-2.8.0.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"Hadoop Combiner\")\n",
    "hdfs = \"hdfs://localhost:9000/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "customer_df = sc.textFile(hdfs + \"/user/askar/join/customer.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1,Habib Nurmagamedov,555-555-555',\n",
       " '2,Tony Ferguson,564-667-789',\n",
       " '3,Jose Aldo,456-543-222',\n",
       " '4,Konor Maccgregor,777-777-111']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orders_df = sc.textFile(hdfs + \"/user/askar/join/orders.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3,A,12.95,02-Jun-2009',\n",
       " '1,B,88.25,20-May-2009',\n",
       " '2,C,32.45,30-Nov-2008',\n",
       " '3,D,25.67,22-Jan-2010']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.hadoop.conf.Configuration;\n",
    "import org.apache.hadoop.conf.Configured;\n",
    "import org.apache.hadoop.contrib.utils.join.DataJoinMapperBase;\n",
    "import org.apache.hadoop.contrib.utils.join.DataJoinReducerBase;\n",
    "import org.apache.hadoop.contrib.utils.join.TaggedMapOutput;\n",
    "import org.apache.hadoop.fs.Path;\n",
    "import org.apache.hadoop.io.Text;\n",
    "import org.apache.hadoop.io.Writable;\n",
    "import org.apache.hadoop.mapred.*;\n",
    "\n",
    "import org.apache.hadoop.util.Tool;\n",
    "import org.apache.hadoop.util.ToolRunner;\n",
    "\n",
    "import java.io.DataInput;\n",
    "import java.io.DataOutput;\n",
    "import java.io.IOException;\n",
    "\n",
    "\n",
    "/**\n",
    " * @author Askar Shabykov\n",
    " * @since 02.06.17\n",
    " */\n",
    "public class DataJoinTask extends Configured implements Tool {\n",
    "\n",
    "    public static class DataJoinMapper extends DataJoinMapperBase {\n",
    "\n",
    "        public Text generateInputTag(String inputFile) {\n",
    "            return new Text(inputFile);\n",
    "        }\n",
    "\n",
    "        public Text generateGroupKey(TaggedMapOutput aRecoder) {\n",
    "            String line = ((Text) aRecoder.getData()).toString();\n",
    "            String[] fields = line.split(\",\");\n",
    "            String groupKey = fields[1];\n",
    "            return new Text(groupKey);\n",
    "        }\n",
    "\n",
    "        public TaggedMapOutput generateTaggedMapOutput(Object value) {\n",
    "            TaggedWritable retv = new TaggedWritable((Text)value);\n",
    "            retv.setTag(this.inputTag);\n",
    "            return retv;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    public static class DataJoinReducer extends DataJoinReducerBase {\n",
    "        public TaggedMapOutput combine(Object[] tags, Object[] values) {\n",
    "            if (tags.length < 2) return null;\n",
    "            String joinStr = \"\";\n",
    "            for (int i = 0; i < values.length; i++) {\n",
    "                if (i > 0) joinStr += \",\";\n",
    "                TaggedWritable tw = (TaggedWritable) values[i];\n",
    "                String line = ((Text) tw.getData()).toString();\n",
    "                String[] tokens = line.split(\",\", 2);\n",
    "                joinStr += tokens[1];\n",
    "            }\n",
    "            return new TaggedWritable(new Text(joinStr), (Text) tags[0]);\n",
    "        }\n",
    "\n",
    "    }\n",
    "\n",
    "    public static class TaggedWritable extends TaggedMapOutput {\n",
    "\n",
    "        private Writable data;\n",
    "        private Text tag;\n",
    "\n",
    "        TaggedWritable(Writable data, Text tag) {\n",
    "            this.data = data;\n",
    "            this.tag = tag;\n",
    "\n",
    "        }\n",
    "\n",
    "        TaggedWritable(Writable data) {\n",
    "            this.data = data;\n",
    "            this.tag = new Text(\"\");\n",
    "        }\n",
    "\n",
    "        public Writable getData() {\n",
    "            return data;\n",
    "        }\n",
    "\n",
    "        public Text getTag() {\n",
    "            return tag;\n",
    "        }\n",
    "\n",
    "        public void setData(Writable data) {\n",
    "            this.data = data;\n",
    "        }\n",
    "\n",
    "        public void setTag(Text tag) {\n",
    "            this.tag = tag;\n",
    "        }\n",
    "\n",
    "        public void write(DataOutput dataOutput) throws IOException {\n",
    "            this.tag.write(dataOutput);\n",
    "            this.data.write(dataOutput);\n",
    "        }\n",
    "\n",
    "        public void readFields(DataInput dataInput) throws IOException {\n",
    "            this.tag.readFields(dataInput);\n",
    "            this.data.readFields(dataInput);\n",
    "        }\n",
    "\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "    public int run(String[] args) throws IOException {\n",
    "        Configuration configuration = getConf();\n",
    "        JobConf job = new JobConf(configuration, this.getClass());\n",
    "\n",
    "        Path in_put1 = new Path(\"/user/askar/join/customer.csv\");\n",
    "        Path in_put2 = new Path(\"/user/askar/join/orders.csv\");\n",
    "        Path out_put = new Path(\"/user/askar/output\");\n",
    "\n",
    "        FileInputFormat.addInputPath(job, in_put1);\n",
    "        FileInputFormat.addInputPath(job, in_put2);\n",
    "        FileOutputFormat.setOutputPath(job, out_put);\n",
    "\n",
    "        job.setMapperClass(DataJoinMapper.class); // mapper class\n",
    "        job.setReducerClass(DataJoinReducer.class); // reducer class\n",
    "        job.setInputFormat(TextInputFormat.class);\n",
    "        job.setOutputFormat(TextOutputFormat.class);\n",
    "        job.setOutputKeyClass(Text.class);\n",
    "        job.setOutputValueClass(TaggedWritable.class);\n",
    "        job.set(\"mapred.textoutputformat.separator\", \",\");\n",
    "        JobClient.runJob(job);\n",
    "\n",
    "        return 0;\n",
    "    }\n",
    "\n",
    "    public static void main(String[] args) throws Exception {\n",
    "        int exitCode = ToolRunner.run(new Configuration(), new DataJoinTask(), args);\n",
    "        System.exit(exitCode);\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop jar DataJoin-1.0-SNAPSHOT.jar DataJoinTask\n",
    "    -libjars $HADOOP_HOME/libexec/share/hadoop/tools/lib/hadoop-datajoin-2.8.0.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
