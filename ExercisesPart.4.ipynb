{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises Part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Упражнения по 4 главе "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"Exercises Part 4\")\n",
    "hdfs = \"hdfs://localhost:9000/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exe. 1\n",
    "\n",
    "Вывести k наибольших значений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>\n",
    "Mapper:\n",
    "</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# attribute_k_max_mapper.py \n",
    "import sys\n",
    "import operator\n",
    "\n",
    "index = int(sys.argv[1]) if len(sys.argv) > 1 else 8\n",
    "k = int(sys.argv[2]) if len(sys.argv) > 2 else 5\n",
    "\n",
    "class Mapper:\n",
    "    __max_values_list = []\n",
    "    __last_max_value = 0\n",
    "    \n",
    "    def __init__(self, k, index):\n",
    "        self.k = k\n",
    "        self.index = index\n",
    "    \n",
    "    def map(self, key, value):\n",
    "        fields = value.split(\",\")\n",
    "        if len(fields) > 8:\n",
    "            if fields[self.index].isdigit():\n",
    "                val = int(fields[self.index])\n",
    "                if val > self.__last_max_value:\n",
    "                    self.__last_max_value = val\n",
    "                    self.__max_values_list.append(fields)\n",
    "                    self.__max_values_list = sorted(self.__max_values_list, key=operator.itemgetter(self.index), reverse=True)[:self.k]\n",
    "\n",
    "    def close(self):\n",
    "        for val in self.__max_values_list:\n",
    "            if len(val):\n",
    "                print(','.join(val))\n",
    "\n",
    "mapper = Mapper(k, index)\n",
    "for i, line in enumerate(sys.stdin):\n",
    "    if line.replace(\" \", \"\"):\n",
    "        mapper.map(i, line)\n",
    "mapper.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop jar $HADOOP_HOME/libexec/share/hadoop/tools/lib/hadoop-streaming-2.8.0.jar \\ \n",
    "    -D mapreduce.job.reduces=1 \\ \n",
    "    -input /user/askar/apat63_99.txt \\\n",
    "    -output /user/askar/output \\ \n",
    "    -mapper \"python3 /Users/shabykov/Hadoop/attribute_k_max_mapper.py 8 4\"\n",
    "    -reducer \"python3 /Users/shabykov/Hadoop/attribute_k_max_mapper.py 8 4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5095054', '1992', '11757', '1990', '\"DE\"', '\"\"', '618615', '2', '868', '524', '1', '15', '40', '111', '0.975', '0.8442', '0.8586', '4.1622', '14.45', '0.0571', '0.05', '0.0093', '0.009\\t']\n",
      "['\\t']\n",
      "['3862265', '1975', '5499', '1972', '\"US\"', '\"TX\"', '186590', '2', '80', '525', '1', '15', '7', '106', '1', '0.7517', '0.6122', '15.8491', '5.8571', '0', '0', '0.068', '0.066\\t']\n",
      "['\\t']\n",
      "['3860168', '1975', '5492', '1971', '\"US\"', '\"OH\"', '270680', '2', '72', '702', '2', '22', '8', '4', '1', '0.75', '0.6875', '12', '4.875', '0.5714', '0.5', '0', '0\\t']\n",
      "['\\t']\n",
      "['4085139', '1978', '6682', '1976', '\"US\"', '\"MI\"', '600480', '2', '706', '564', '1', '14', '7', '1', '0.8571', '0', '0.8333', '14', '9.5714', '0.6', '0.4286', '0', '0\\t']\n",
      "['\\t']\n"
     ]
    }
   ],
   "source": [
    "out_data = sc.textFile(hdfs + \"/user/askar/output/part-00000\")\n",
    "\n",
    "for val in out_data.collect():\n",
    "    print(val.split(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exe. 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
