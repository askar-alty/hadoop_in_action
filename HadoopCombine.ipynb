{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Hadoop Combiner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "   Используется для уменьшениы выходных данных маппера, тем самым уменьшает сетевую нагрузки между мапперами и редьюсерами. В основном применяется для неравномерно распределенных данных, и тех данных, у которых количество значений соотвествующих определенному ключу очень много, проядка 100000 или больше. И редьюсер реализует дистрибутивную функцию, например, вычисление максимума, минимума или суммирование значений. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"Hadoop Combiner\")\n",
    "hdfs = \"hdfs://localhost:9000/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Пример № 1. Реализация MapReduce-задачи в интерфейсе Hadoop Streaming. Вычисление средного значения количества пунктов в формуле патента  патентообладателей определенной страны.\n",
    "Это пример неравномерного распределения данных, так как больше всего патентов за период 1960 - 2000 было поданно США. В маппере возмем поле стран за ключи, значения за 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# average_by_attribute_mapper.py\n",
    "\n",
    "import sys\n",
    "\n",
    "contry_index = 4 # CONTRY - страна патентаобладателя\n",
    "claims_index = 8 # CLAIMS - количество пунктов в формуле патента\n",
    "\n",
    "for line in sys.stdin:\n",
    "    fields = line.split(\",\")\n",
    "    if fields[claims_index] and fields[claims_index].isdigit():\n",
    "        print(fields[contry_index][1:-1] + '\\t' + fields[claims_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hadoop jar $HADOOP_HOME/libexec/share/hadoop/tools/lib/hadoop-streaming-2.8.0.jar \\ \n",
    "    -D mapreduce.job.reduces=1 \\ \n",
    "    -input /user/askar/apat63_99.txt \\\n",
    "    -output /user/askar/output \\ \n",
    "    -mapper \"python3 /Users/shabykov/Hadoop/average_by_attribute_mapper.py\" \\\n",
    "    -file /Users/shabykov/Hadoop/average_by_attribute_mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "out_data = sc.textFile(hdfs + \"/user/askar/output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AD\t7\n",
      "AD\t14\n",
      "AD\t28\n",
      "AD\t12\n",
      "AD\t9\n",
      "AE\t4\n",
      "AE\t12\n",
      "AE\t24\n",
      "AE\t16\n",
      "AE\t11\n",
      "AE\t35\n",
      "AE\t16\n",
      "AE\t20\n",
      "AE\t10\n",
      "AE\t7\n",
      "AE\t23\n",
      "AE\t26\n",
      "AE\t11\n",
      "AE\t12\n",
      "AE\t4\n",
      "AG\t20\n",
      "AG\t7\n",
      "AG\t8\n",
      "AG\t12\n",
      "AG\t3\n",
      "AG\t24\n",
      "AG\t14\n",
      "AG\t18\n",
      "AI\t10\n",
      "AM\t18\n",
      "AN\t3\n",
      "AN\t26\n"
     ]
    }
   ],
   "source": [
    "for val in out_data.collect()[:32]:\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Reducer считает среднее значение данных по опредделенному ключу\n",
    "Combiner = Reducer (в качестве комбайнера будем использовать редьюсер)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# average_by_attribute_reducer.py\n",
    "\n",
    "import sys\n",
    "\n",
    "last_key, count, summ = None, 0, 0.0\n",
    "for line in sys.stdin:\n",
    "    (key, val) = line.split(\"\\t\")\n",
    "    \n",
    "    if last_key and last_key != key:\n",
    "        print(\"{}\\t{}\".format(last_key, summ / count))\n",
    "        summ, count = 0.0, 0\n",
    "    \n",
    "    last_key = key\n",
    "    summ += float(val)\n",
    "    count += 1\n",
    "\n",
    "if last_key:\n",
    "    print(\"{}\\t{}\".format(last_key, summ / count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hadoop jar $HADOOP_HOME/libexec/share/hadoop/tools/lib/hadoop-streaming-2.8.0.jar \\ \n",
    "    -D mapreduce.job.reduces=1 \\ \n",
    "    -input /user/askar/apat63_99.txt \\\n",
    "    -output /user/askar/output \\ \n",
    "    -mapper \"python3 /Users/shabykov/Hadoop/average_by_attribute_mapper.py\" \\\n",
    "    -combiner \"python3 /Users/shabykov/Hadoop/average_by_attribute_reducer.py\" \\\n",
    "    -file /Users/shabykov/Hadoop/average_by_attribute_mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "out_data = sc.textFile(hdfs + \"/user/askar/output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Результаты на выходе комбайнеров: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AD\t7.0\n",
      "AD\t15.75\n",
      "AE\t15.5\n",
      "AE\t15.2\n",
      "AG\t13.5\n",
      "AG\t13.166666666666666\n",
      "AI\t10.0\n",
      "AM\t18.0\n",
      "AN\t11.333333333333334\n",
      "AN\t4.5\n",
      "AR\t8.191489361702128\n",
      "AR\t10.258555133079849\n",
      "AT\t12.070684314277669\n",
      "AT\t9.46241627387745\n",
      "AU\t10.206333973128599\n",
      "AU\t14.1228402865571\n",
      "AW\t15.5\n",
      "AZ\t11.0\n",
      "BB\t13.0\n",
      "BB\t10.666666666666666\n",
      "BE\t11.14691943127962\n",
      "BE\t12.64612374663732\n",
      "BG\t5.68\n",
      "BG\t4.829721362229102\n",
      "BH\t5.0\n",
      "BH\t8.0\n",
      "BM\t10.8\n",
      "BM\t9.625\n",
      "BN\t9.0\n",
      "BO\t8.777777777777779\n",
      "BO\t20.666666666666668\n",
      "BR\t10.23339317773788\n",
      "BR\t7.894894894894895\n",
      "BS\t13.783333333333333\n",
      "BS\t18.5\n",
      "BY\t15.0\n",
      "BZ\t28.0\n",
      "CA\t10.516544655929723\n",
      "CA\t13.705782951558392\n",
      "CC\t9.0\n",
      "CD\t14.0\n",
      "CD\t6.0\n",
      "CH\t10.744842439356155\n",
      "CH\t12.919727088948788\n",
      "CI\t7.666666666666667\n",
      "CK\t8.0\n",
      "CL\t15.582089552238806\n",
      "CL\t11.4\n",
      "CN\t13.222222222222221\n",
      "CN\t10.862003780718336\n"
     ]
    }
   ],
   "source": [
    "for val in out_data.collect()[:50]:\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hadoop jar $HADOOP_HOME/libexec/share/hadoop/tools/lib/hadoop-streaming-2.8.0.jar \\ \n",
    "    -D mapreduce.job.reduces=1 \\ \n",
    "    -input /user/askar/apat63_99.txt \\\n",
    "    -output /user/askar/output \\ \n",
    "    -mapper \"python3 /Users/shabykov/Hadoop/average_by_attribute_mapper.py\" \\\n",
    "    -combiner \"python3 /Users/shabykov/Hadoop/average_by_attribute_reducer.py\"\\\n",
    "    -reducer \"python3 /Users/shabykov/Hadoop/average_by_attribute_reducer.py\"\\ \n",
    "    -file /Users/shabykov/Hadoop/average_by_attribute_mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Результаты на выходе редьюсеров после использования комбайнеров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "out_data = sc.textFile(hdfs + \"/user/askar/output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AD\t11.375\n",
      "AE\t15.35\n",
      "AG\t13.333333333333332\n",
      "AI\t10.0\n",
      "AM\t18.0\n",
      "AN\t7.916666666666667\n",
      "AR\t9.225022247390989\n",
      "AT\t10.76655029407756\n",
      "AU\t12.16458712984285\n",
      "AW\t15.5\n",
      "AZ\t11.0\n",
      "BB\t11.833333333333332\n",
      "BE\t11.896521588958471\n",
      "BG\t5.25486068111455\n",
      "BH\t6.5\n",
      "BM\t10.2125\n",
      "BN\t9.0\n",
      "BO\t14.722222222222223\n",
      "BR\t9.064144036316389\n",
      "BS\t16.141666666666666\n",
      "BY\t15.0\n",
      "BZ\t28.0\n",
      "CA\t12.111163803744057\n",
      "CC\t9.0\n",
      "CD\t10.0\n",
      "CH\t11.832284764152472\n",
      "CI\t7.666666666666667\n",
      "CK\t8.0\n",
      "CL\t13.491044776119402\n",
      "CN\t12.042113001470279\n",
      "CO\t12.044117647058822\n",
      "CR\t11.616666666666667\n",
      "CS\t7.60668016194332\n",
      "CU\t9.37784090909091\n",
      "CY\t10.785714285714286\n",
      "CZ\t12.823529411764707\n",
      "DE\t11.131700257745276\n",
      "DK\t10.832334668499442\n",
      "DO\t10.642857142857142\n",
      "DZ\t14.0\n",
      "EC\t12.13888888888889\n",
      "EE\t17.0\n",
      "EG\t11.73970588235294\n",
      "ES\t8.6256708539627\n",
      "FI\t10.248721177385207\n",
      "FO\t8.0\n",
      "FR\t10.97657827839095\n",
      "GB\t11.503545243661414\n",
      "GE\t7.5\n",
      "GF\t10.0\n"
     ]
    }
   ],
   "source": [
    "for val in out_data.collect()[:50]:\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "К сожалению, ответ не соотвествует реальности, так как редьюсер реализует недистрибутивную функцию. Необходимо поменять логику работы мапперов и написать отдельный комбайнер. В новый распеределитель на выходе будет выдавать: \n",
    "\n",
    "    - key - страна\n",
    "    - value - значение количества пунктов, количество таких записей.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h5>Mapper:</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# average_by_attribute_mapper.py\n",
    "\n",
    "import sys\n",
    "\n",
    "contry_index = 4 # CONTRY - страна патентаобладателя\n",
    "claims_index = 8 # CLAIMS - количество пунктов в формуле патента\n",
    "\n",
    "for line in sys.stdin:\n",
    "    fields = line.split(\",\")\n",
    "    if fields[claims_index] and fields[claims_index].isdigit():\n",
    "        print(\"{}\\t{},{}\".format(fields[contry_index][1:-1], fields[claims_index], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h5>Combiner:</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# average_by_attribute_combiner.py\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "last_key, count, summ = None, 0, 0.0\n",
    "for line in sys.stdin:\n",
    "    (key, value) = line.split(\"\\t\")\n",
    "    (s, c) = value.split(\",\")\n",
    "    \n",
    "    if last_key and last_key != key:\n",
    "        print(\"{}\\t{},{}\".format(last_key, summ, count))\n",
    "        summ, count = 0.0, 0\n",
    "    \n",
    "    last_key = key\n",
    "    summ += float(s)\n",
    "    count += int(c)\n",
    "\n",
    "if last_key:\n",
    "    print(\"{}\\t{},{}\".format(last_key, summ, count))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h5>Reducer:</5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# average_by_attribute_reducer.py\n",
    "\n",
    "import sys\n",
    "\n",
    "last_key, count, summ = None, 0, 0.0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    (key, value) = line.split(\"\\t\")\n",
    "    (s, c) = value.split(\",\")\n",
    "    \n",
    "    if last_key and last_key != key:\n",
    "        print(\"{}\\t{}\".format(last_key, summ / count))\n",
    "        summ, count = 0.0, 0\n",
    "    \n",
    "    last_key = key\n",
    "    summ += float(s)\n",
    "    count += int(c)\n",
    "\n",
    "if last_key:\n",
    "    print(\"{}\\t{}\".format(last_key, summ / count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hadoop jar $HADOOP_HOME/libexec/share/hadoop/tools/lib/hadoop-streaming-2.8.0.jar \\ \n",
    "    -D mapreduce.job.reduces=1 \\ \n",
    "    -input /user/askar/apat63_99.txt \\\n",
    "    -output /user/askar/output \\ \n",
    "    -mapper \"python3 /Users/shabykov/Hadoop/average_by_attribute_mapper.py\" \\\n",
    "    -combiner \"python3 /Users/shabykov/Hadoop/average_by_attribute_combiner.py\"\\\n",
    "    -reducer \"python3 /Users/shabykov/Hadoop/average_by_attribute_reducer.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "out_data = sc.textFile(hdfs + \"/user/askar/output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AD\t14.0\n",
      "AE\t15.4\n",
      "AG\t13.25\n",
      "AI\t10.0\n",
      "AM\t18.0\n",
      "AN\t9.625\n",
      "AR\t9.188990825688073\n",
      "AT\t10.683988393563704\n",
      "AU\t12.291563832174107\n",
      "AW\t15.5\n",
      "AZ\t11.0\n",
      "BB\t11.0\n",
      "BE\t11.945544554455445\n",
      "BG\t4.989949748743719\n",
      "BH\t6.5\n",
      "BM\t10.076923076923077\n",
      "BN\t9.0\n",
      "BO\t11.75\n",
      "BR\t9.358426966292134\n",
      "BS\t15.778846153846153\n",
      "BY\t15.0\n",
      "BZ\t28.0\n",
      "CA\t12.286875635112953\n",
      "CC\t9.0\n",
      "CD\t10.0\n",
      "CH\t11.619630031169535\n",
      "CI\t7.666666666666667\n",
      "CK\t8.0\n",
      "CL\t13.901785714285714\n",
      "CN\t11.23407643312102\n",
      "CO\t12.057142857142857\n",
      "CR\t11.454545454545455\n",
      "CS\t6.77487922705314\n",
      "CU\t10.222222222222221\n",
      "CY\t11.619047619047619\n",
      "CZ\t12.823529411764707\n",
      "DE\t11.051392049883086\n",
      "DK\t10.903726708074535\n",
      "DO\t10.642857142857142\n",
      "DZ\t14.0\n",
      "EC\t12.476190476190476\n",
      "EE\t17.0\n",
      "EG\t11.91891891891892\n",
      "ES\t8.733971997052322\n",
      "FI\t10.738573407202216\n",
      "FO\t8.0\n",
      "FR\t10.943602074030457\n",
      "GB\t11.322297099326105\n",
      "GE\t7.5\n",
      "GF\t10.0\n"
     ]
    }
   ],
   "source": [
    "for val in out_data.collect()[:50]:\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Получили правильные результаты! \n",
    "\n",
    "ВАЖНО СЛЕДИТЬ ЗА ДАННЫМИ НА ВЫХОДЕ МАППЕРОВ И КОМБАЙНЕРОВ ОНИ ДОЛЖНЫ БЫТЬ ОДНОГО ТИПА ИЛИ ФОРМАТА! Так-как комбайнер может выполняться так и не выполняться, все зависить от обьема данных по определенному ключу. Это проще отследить в Java API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Пример № 1. Реализация MapReduce-задачи в интерфейсе Java API. Вычисление средного значения количества пунктов в формуле патента  патентообладателей определенной страны.\n",
    "Это пример неравномерного распределения данных, так как больше всего патентов за период 1960 - 2000 было поданно США. В маппере возмем поле стран за ключи, значения за 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h5>\n",
    "Код реализации (AverageByAttribute.java): \n",
    "</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import org.apache.hadoop.conf.Configuration;\n",
    "import org.apache.hadoop.conf.Configured;\n",
    "import org.apache.hadoop.fs.Path;\n",
    "import org.apache.hadoop.io.DoubleWritable;\n",
    "import org.apache.hadoop.io.IntWritable;\n",
    "import org.apache.hadoop.io.LongWritable;\n",
    "import org.apache.hadoop.io.Text;\n",
    "import org.apache.hadoop.mapreduce.Job;\n",
    "import org.apache.hadoop.mapreduce.Mapper;\n",
    "import org.apache.hadoop.mapreduce.Reducer;\n",
    "import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;\n",
    "import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;\n",
    "import org.apache.hadoop.util.Tool;\n",
    "import org.apache.hadoop.util.ToolRunner;\n",
    "\n",
    "import java.io.IOException;\n",
    "\n",
    "/**\n",
    " * @author Askar Shabykov\n",
    " * @since 01.06.17\n",
    " */\n",
    "\n",
    "public class AverageByAttribute extends Configured implements Tool {\n",
    "\n",
    "\n",
    "    // Mapper\n",
    "    public static class AverageByAttributeMapper extends Mapper<LongWritable, Text, Text, Text> {\n",
    "\n",
    "        public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n",
    "\n",
    "            String[] fields = value.toString().split(\",\", -20);\n",
    "            String country = fields[4];\n",
    "            String numClaims = fields[8];\n",
    "            if (numClaims.length() > 0 && !numClaims.startsWith(\"\\\"\")) {\n",
    "                context.write(new Text(country), new Text(numClaims + \",1\"));\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "    // Combiner\n",
    "    public static class AverageByAttributeCombiner extends Reducer<Text, Text, Text, Text> {\n",
    "        public void reduce(Text key, Iterable<Text> values, Context context) throws IOException, InterruptedException {\n",
    "\n",
    "            double sum = 0;\n",
    "            int count = 0;\n",
    "            for (Text value : values) {\n",
    "                String[] fields = value.toString().split(\",\");\n",
    "                sum += Double.parseDouble(fields[0]);\n",
    "                count += Integer.parseInt(fields[1]);\n",
    "            }\n",
    "            context.write(key, new Text(String.valueOf(sum) + \",\" + String.valueOf(count)));\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "    // Reducer\n",
    "    public static class AverageByAttributeReducer extends Reducer<Text, Text, Text, DoubleWritable> {\n",
    "        public void reduce(Text key, Iterable<Text> values, Context context) throws IOException, InterruptedException {\n",
    "            double sum = 0;\n",
    "            int count = 0;\n",
    "            for (Text value : values) {\n",
    "                String[] fields = value.toString().split(\",\");\n",
    "                sum += Double.parseDouble(fields[0]);\n",
    "                count += Integer.parseInt(fields[1]);\n",
    "            }\n",
    "            context.write(key, new DoubleWritable(sum / count));\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "    public int run(String[] args) throws IOException, ClassNotFoundException, InterruptedException {\n",
    "        Configuration configuration = getConf(); // конфигурации воркера\n",
    "\n",
    "        Job job = Job.getInstance(configuration, \"Average By Attribute\"); /// создаем воркера\n",
    "        job.setJarByClass(this.getClass());\n",
    "\n",
    "        Path in_put = new Path(args[0]); // путь к входным файлам в hdfs\n",
    "        Path out_put = new Path(args[1]); // путь к выходному файлу в hdfs\n",
    "\n",
    "        TextInputFormat.addInputPath(job, in_put);\n",
    "        TextOutputFormat.setOutputPath(job, out_put);\n",
    "\n",
    "        job.setMapperClass(AverageByAttributeMapper.class); // mapper class\n",
    "        job.setCombinerClass(AverageByAttributeCombiner.class); // combiner class\n",
    "        job.setReducerClass(AverageByAttributeReducer.class); // reducer class\n",
    "\n",
    "        job.setInputFormatClass(TextInputFormat.class);\n",
    "        job.setOutputFormatClass(TextOutputFormat.class);\n",
    "\n",
    "        job.setOutputKeyClass(Text.class); // тип ключа на выходе маппера\n",
    "        job.setOutputValueClass(Text.class); // тип значения на выходе маппера\n",
    "\n",
    "        return job.waitForCompletion(true) ? 0 : 1;\n",
    "    }\n",
    "\n",
    "    public static void main(String[] args) throws Exception {\n",
    "        int exitCode = ToolRunner.run(new Configuration(), new AverageByAttribute(), args);\n",
    "        System.exit(exitCode);\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h5>\n",
    "pom.xml\n",
    "</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n",
    "         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n",
    "         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n",
    "    <modelVersion>4.0.0</modelVersion>\n",
    "\n",
    "    <groupId>AverageByAttribute</groupId>\n",
    "    <artifactId>AverageByAttribute</artifactId>\n",
    "    <version>1.0-SNAPSHOT</version>\n",
    "\n",
    "    <dependencies>\n",
    "        <dependency>\n",
    "            <groupId>org.apache.hadoop</groupId>\n",
    "            <artifactId>hadoop-client</artifactId>\n",
    "            <version>2.8.0</version>\n",
    "            <scope>provided</scope>\n",
    "        </dependency>\n",
    "    </dependencies>\n",
    "\n",
    "</project>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    " hadoop jar AverageByAttribute-1.0-SNAPSHOT.jar \\\n",
    "    AverageByAttribute \\\n",
    "    /user/askar/apat63_99.txt \\\n",
    "    /user/askar/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "out_data = sc.textFile(hdfs + \"/user/askar/output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"AD\"\t14.0\n",
      "\"AE\"\t15.4\n",
      "\"AG\"\t13.25\n",
      "\"AI\"\t10.0\n",
      "\"AM\"\t18.0\n",
      "\"AN\"\t9.625\n",
      "\"AR\"\t9.188990825688073\n",
      "\"AT\"\t10.683988393563704\n",
      "\"AU\"\t12.291563832174107\n",
      "\"AW\"\t15.5\n",
      "\"AZ\"\t11.0\n",
      "\"BB\"\t11.0\n",
      "\"BE\"\t11.945544554455445\n",
      "\"BG\"\t4.989949748743719\n",
      "\"BH\"\t6.5\n",
      "\"BM\"\t10.076923076923077\n",
      "\"BN\"\t9.0\n",
      "\"BO\"\t11.75\n",
      "\"BR\"\t9.358426966292134\n",
      "\"BS\"\t15.778846153846153\n",
      "\"BY\"\t15.0\n",
      "\"BZ\"\t28.0\n",
      "\"CA\"\t12.286875635112953\n",
      "\"CC\"\t9.0\n",
      "\"CD\"\t10.0\n",
      "\"CH\"\t11.619630031169535\n",
      "\"CI\"\t7.666666666666667\n",
      "\"CK\"\t8.0\n",
      "\"CL\"\t13.901785714285714\n",
      "\"CN\"\t11.23407643312102\n",
      "\"CO\"\t12.057142857142857\n",
      "\"CR\"\t11.454545454545455\n",
      "\"CS\"\t6.77487922705314\n",
      "\"CU\"\t10.222222222222221\n",
      "\"CY\"\t11.619047619047619\n",
      "\"CZ\"\t12.823529411764707\n",
      "\"DE\"\t11.051392049883086\n",
      "\"DK\"\t10.903726708074535\n",
      "\"DO\"\t10.642857142857142\n",
      "\"DZ\"\t14.0\n",
      "\"EC\"\t12.476190476190476\n",
      "\"EE\"\t17.0\n",
      "\"EG\"\t11.91891891891892\n",
      "\"ES\"\t8.733971997052322\n",
      "\"FI\"\t10.738573407202216\n",
      "\"FO\"\t8.0\n",
      "\"FR\"\t10.943602074030457\n",
      "\"GB\"\t11.322297099326105\n",
      "\"GE\"\t7.5\n",
      "\"GF\"\t10.0\n"
     ]
    }
   ],
   "source": [
    "for val in out_data.collect()[:50]:\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
