{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop Combiner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используется для уменьшениы выходных данных маппера, тем самым уменьшает сетевую нагрузки между мапперами и редьюсерами. В основном применяется для неравномерно распределенных данных, и тех данных, у которых количество значений соотвествующих определенному ключу очень много, проядка 100000 или больше. И редьюсер реализует дистрибутивную функцию, например, вычисление максимума, минимума или суммирование значений. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"Hadoop Combiner\")\n",
    "hdfs = \"hdfs://localhost:9000/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример № 1. Реализация MapReduce-задачи в интерфейсе Hadoop Streaming. Вычисление средного значения количества пунктов в формуле патента  патентообладателей определенной страны.\n",
    "Это пример неравномерного распределения данных, так как больше всего патентов за период 1960 - 2000 было поданно США. В маппере возмем поле стран за ключи, значения за 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# average_by_attribute_mapper.py\n",
    "\n",
    "import sys\n",
    "\n",
    "contry_index = 4 # CONTRY - страна патентаобладателя\n",
    "claims_index = 8 # CLAIMS - количество пунктов в формуле патента\n",
    "\n",
    "for line in sys.stdin:\n",
    "    fields = line.split(\",\")\n",
    "    if fields[claims_index] and fields[claims_index].isdigit():\n",
    "        print(fields[contry_index][1:-1] + '\\t' + fields[claims_index])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "hadoop jar $HADOOP_HOME/libexec/share/hadoop/tools/lib/hadoop-streaming-2.8.0.jar \\ \n",
    "    -D mapreduce.job.reduces=1 \\ \n",
    "    -input /user/askar/apat63_99.txt \\\n",
    "    -output /user/askar/output \\ \n",
    "    -mapper \"python3 /Users/shabykov/Hadoop/average_by_attribute_mapper.py\" \\\n",
    "    -file /Users/shabykov/Hadoop/average_by_attribute_mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_data = sc.textFile(hdfs + \"/user/askar/output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AD\t7\n",
      "AD\t14\n",
      "AD\t28\n",
      "AD\t12\n",
      "AD\t9\n",
      "AE\t4\n",
      "AE\t12\n",
      "AE\t24\n",
      "AE\t16\n",
      "AE\t11\n",
      "AE\t35\n",
      "AE\t16\n",
      "AE\t20\n",
      "AE\t10\n",
      "AE\t7\n",
      "AE\t23\n",
      "AE\t26\n",
      "AE\t11\n",
      "AE\t12\n",
      "AE\t4\n",
      "AG\t20\n",
      "AG\t7\n",
      "AG\t8\n",
      "AG\t12\n",
      "AG\t3\n",
      "AG\t24\n",
      "AG\t14\n",
      "AG\t18\n",
      "AI\t10\n",
      "AM\t18\n",
      "AN\t3\n",
      "AN\t26\n"
     ]
    }
   ],
   "source": [
    "for val in out_data.collect()[:32]:\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducer считает среднее значение данных по опредделенному ключу\n",
    "Combiner = Reducer (в качестве комбайнера будем использовать редьюсер)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# average_by_attribute_reducer.py\n",
    "\n",
    "import sys\n",
    "\n",
    "last_key, count, summ = None, 0, 0.0\n",
    "for line in sys.stdin:\n",
    "    (key, val) = line.split(\"\\t\")\n",
    "    \n",
    "    if last_key and last_key != key:\n",
    "        print(\"{}\\t{}\".format(last_key, summ / count))\n",
    "        summ, count = 0.0, 0\n",
    "    \n",
    "    last_key = key\n",
    "    summ += float(val)\n",
    "    count += 1\n",
    "\n",
    "if last_key:\n",
    "    print(\"{}\\t{}\".format(last_key, summ / count))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "hadoop jar $HADOOP_HOME/libexec/share/hadoop/tools/lib/hadoop-streaming-2.8.0.jar \\ \n",
    "    -D mapreduce.job.reduces=1 \\ \n",
    "    -input /user/askar/apat63_99.txt \\\n",
    "    -output /user/askar/output \\ \n",
    "    -mapper \"python3 /Users/shabykov/Hadoop/average_by_attribute_mapper.py\" \\\n",
    "    -combiner \"python3 /Users/shabykov/Hadoop/average_by_attribute_reducer.py\" \\\n",
    "    -file /Users/shabykov/Hadoop/average_by_attribute_mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_data = sc.textFile(hdfs + \"/user/askar/output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты на выходе комбайнеров: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AD\t7.0\n",
      "AD\t15.75\n",
      "AE\t15.5\n",
      "AE\t15.2\n",
      "AG\t13.5\n",
      "AG\t13.166666666666666\n",
      "AI\t10.0\n",
      "AM\t18.0\n",
      "AN\t11.333333333333334\n",
      "AN\t4.5\n",
      "AR\t8.191489361702128\n",
      "AR\t10.258555133079849\n",
      "AT\t12.070684314277669\n",
      "AT\t9.46241627387745\n",
      "AU\t10.206333973128599\n",
      "AU\t14.1228402865571\n",
      "AW\t15.5\n",
      "AZ\t11.0\n",
      "BB\t13.0\n",
      "BB\t10.666666666666666\n",
      "BE\t11.14691943127962\n",
      "BE\t12.64612374663732\n",
      "BG\t5.68\n",
      "BG\t4.829721362229102\n",
      "BH\t5.0\n",
      "BH\t8.0\n",
      "BM\t10.8\n",
      "BM\t9.625\n",
      "BN\t9.0\n",
      "BO\t8.777777777777779\n",
      "BO\t20.666666666666668\n",
      "BR\t10.23339317773788\n",
      "BR\t7.894894894894895\n",
      "BS\t13.783333333333333\n",
      "BS\t18.5\n",
      "BY\t15.0\n",
      "BZ\t28.0\n",
      "CA\t10.516544655929723\n",
      "CA\t13.705782951558392\n",
      "CC\t9.0\n",
      "CD\t14.0\n",
      "CD\t6.0\n",
      "CH\t10.744842439356155\n",
      "CH\t12.919727088948788\n",
      "CI\t7.666666666666667\n",
      "CK\t8.0\n",
      "CL\t15.582089552238806\n",
      "CL\t11.4\n",
      "CN\t13.222222222222221\n",
      "CN\t10.862003780718336\n"
     ]
    }
   ],
   "source": [
    "for val in out_data.collect()[:50]:\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "hadoop jar $HADOOP_HOME/libexec/share/hadoop/tools/lib/hadoop-streaming-2.8.0.jar \\ \n",
    "    -D mapreduce.job.reduces=1 \\ \n",
    "    -input /user/askar/apat63_99.txt \\\n",
    "    -output /user/askar/output \\ \n",
    "    -mapper \"python3 /Users/shabykov/Hadoop/average_by_attribute_mapper.py\" \\\n",
    "    -combiner \"python3 /Users/shabykov/Hadoop/average_by_attribute_reducer.py\"\\\n",
    "    -reducer \"python3 /Users/shabykov/Hadoop/average_by_attribute_reducer.py\"\\ \n",
    "    -file /Users/shabykov/Hadoop/average_by_attribute_mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты на выходе редьюсеров после использования комбайнеров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_data = sc.textFile(hdfs + \"/user/askar/output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AD\t11.375\n",
      "AE\t15.35\n",
      "AG\t13.333333333333332\n",
      "AI\t10.0\n",
      "AM\t18.0\n",
      "AN\t7.916666666666667\n",
      "AR\t9.225022247390989\n",
      "AT\t10.76655029407756\n",
      "AU\t12.16458712984285\n",
      "AW\t15.5\n",
      "AZ\t11.0\n",
      "BB\t11.833333333333332\n",
      "BE\t11.896521588958471\n",
      "BG\t5.25486068111455\n",
      "BH\t6.5\n",
      "BM\t10.2125\n",
      "BN\t9.0\n",
      "BO\t14.722222222222223\n",
      "BR\t9.064144036316389\n",
      "BS\t16.141666666666666\n",
      "BY\t15.0\n",
      "BZ\t28.0\n",
      "CA\t12.111163803744057\n",
      "CC\t9.0\n",
      "CD\t10.0\n",
      "CH\t11.832284764152472\n",
      "CI\t7.666666666666667\n",
      "CK\t8.0\n",
      "CL\t13.491044776119402\n",
      "CN\t12.042113001470279\n",
      "CO\t12.044117647058822\n",
      "CR\t11.616666666666667\n",
      "CS\t7.60668016194332\n",
      "CU\t9.37784090909091\n",
      "CY\t10.785714285714286\n",
      "CZ\t12.823529411764707\n",
      "DE\t11.131700257745276\n",
      "DK\t10.832334668499442\n",
      "DO\t10.642857142857142\n",
      "DZ\t14.0\n",
      "EC\t12.13888888888889\n",
      "EE\t17.0\n",
      "EG\t11.73970588235294\n",
      "ES\t8.6256708539627\n",
      "FI\t10.248721177385207\n",
      "FO\t8.0\n",
      "FR\t10.97657827839095\n",
      "GB\t11.503545243661414\n",
      "GE\t7.5\n",
      "GF\t10.0\n"
     ]
    }
   ],
   "source": [
    "for val in out_data.collect()[:50]:\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К сожалению, ответ не соотвествует реальности, так как редьюсер реализует недистрибутивную функцию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример № 1. Реализация MapReduce-задачи в интерфейсе Java API. Вычисление средного значения количества пунктов в формуле патента  патентообладателей определенной страны.\n",
    "Это пример неравномерного распределения данных, так как больше всего патентов за период 1960 - 2000 было поданно США. В маппере возмем поле стран за ключи, значения за 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
